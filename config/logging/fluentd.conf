# Fluentd 日志收集配置

# 系统配置
<system>
  log_level info
  suppress_repeated_stacktrace true
  emit_error_log_interval 30s
  suppress_config_dump
  without_source
</system>

# 输入源配置
<source>
  @type tail
  @id application_logs
  path /var/log/app/*.log
  pos_file /var/log/fluentd/app.log.pos
  tag app.logs
  <parse>
    @type json
    time_key timestamp
    time_format %Y-%m-%dT%H:%M:%S.%L%z
  </parse>
  refresh_interval 5
</source>

<source>
  @type tail
  @id nginx_access_logs
  path /var/log/nginx/access.log
  pos_file /var/log/fluentd/nginx_access.log.pos
  tag nginx.access
  <parse>
    @type nginx
  </parse>
</source>

<source>
  @type tail
  @id nginx_error_logs
  path /var/log/nginx/error.log
  pos_file /var/log/fluentd/nginx_error.log.pos
  tag nginx.error
  <parse>
    @type multiline
    format_firstline /^\d{4}/\d{2}/\d{2} \d{2}:\d{2}:\d{2}/
    format1 /^(?<time>\d{4}/\d{2}/\d{2} \d{2}:\d{2}:\d{2}) \[(?<log_level>\w+)\] (?<pid>\d+).(?<tid>\d+): (?<message>.*)/
  </parse>
</source>

<source>
  @type tail
  @id postgres_logs
  path /var/log/postgresql/*.log
  pos_file /var/log/fluentd/postgres.log.pos
  tag postgres.logs
  <parse>
    @type multiline
    format_firstline /^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}/
    format1 /^(?<time>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}.\d{3}) (?<timezone>\w+) \[(?<pid>\d+)\] (?<level>\w+):  (?<message>.*)/
  </parse>
</source>

<source>
  @type tail
  @id redis_logs
  path /var/log/redis/*.log
  pos_file /var/log/fluentd/redis.log.pos
  tag redis.logs
  <parse>
    @type multiline
    format_firstline /^\d+:\w+ \d{2} \w{3} \d{4} \d{2}:\d{2}:\d{2}/
    format1 /^(?<pid>\d+):(?<role>\w+) (?<time>\d{2} \w{3} \d{4} \d{2}:\d{2}:\d{2}.\d{3}) (?<level>.) (?<message>.*)/
  </parse>
</source>

# Docker 容器日志
<source>
  @type forward
  @id docker_logs
  port 24224
  bind 0.0.0.0
</source>

# 系统日志
<source>
  @type systemd
  @id systemd_logs
  tag systemd
  path /var/log/journal
  <storage>
    @type local
    persistent true
    path /var/log/fluentd/systemd.pos
  </storage>
  <entry>
    field_map {"MESSAGE": "message", "_HOSTNAME": "hostname", "_SYSTEMD_UNIT": "unit"}
  </entry>
</source>

# 过滤器配置
<filter app.logs>
  @type record_transformer
  <record>
    hostname "#{Socket.gethostname}"
    service "secure-frontend"
    environment "#{ENV['ENVIRONMENT'] || 'production'}"
  </record>
</filter>

<filter nginx.**>
  @type record_transformer
  <record>
    hostname "#{Socket.gethostname}"
    service "nginx"
    environment "#{ENV['ENVIRONMENT'] || 'production'}"
  </record>
</filter>

<filter postgres.logs>
  @type record_transformer
  <record>
    hostname "#{Socket.gethostname}"
    service "postgresql"
    environment "#{ENV['ENVIRONMENT'] || 'production'}"
  </record>
</filter>

<filter redis.logs>
  @type record_transformer
  <record>
    hostname "#{Socket.gethostname}"
    service "redis"
    environment "#{ENV['ENVIRONMENT'] || 'production'}"
  </record>
</filter>

# 错误日志特殊处理
<filter **>
  @type grep
  <regexp>
    key level
    pattern ^(ERROR|FATAL|CRITICAL)$
  </regexp>
  <record>
    alert_level "high"
  </record>
</filter>

# 安全事件检测
<filter app.logs>
  @type grep
  <regexp>
    key message
    pattern (login.*failed|unauthorized|security|breach|attack)
  </regexp>
  <record>
    security_event true
    alert_level "critical"
  </record>
</filter>

# 性能监控日志
<filter app.logs>
  @type grep
  <regexp>
    key message
    pattern (slow.*query|timeout|performance|latency)
  </regexp>
  <record>
    performance_issue true
    alert_level "medium"
  </record>
</filter>

# 输出配置
<match app.logs>
  @type elasticsearch
  @id elasticsearch_app
  host elasticsearch
  port 9200
  index_name app-logs
  type_name _doc
  logstash_format true
  logstash_prefix app-logs
  logstash_dateformat %Y.%m.%d
  include_tag_key true
  tag_key @log_name
  flush_interval 10s
  <buffer>
    @type file
    path /var/log/fluentd/app_buffer
    flush_mode interval
    retry_type exponential_backoff
    flush_thread_count 2
    flush_interval 5s
    retry_forever
    retry_max_interval 30
    chunk_limit_size 2M
    queue_limit_length 8
    overflow_action block
  </buffer>
</match>

<match nginx.**>
  @type elasticsearch
  @id elasticsearch_nginx
  host elasticsearch
  port 9200
  index_name nginx-logs
  type_name _doc
  logstash_format true
  logstash_prefix nginx-logs
  logstash_dateformat %Y.%m.%d
  include_tag_key true
  tag_key @log_name
  flush_interval 10s
  <buffer>
    @type file
    path /var/log/fluentd/nginx_buffer
    flush_mode interval
    retry_type exponential_backoff
    flush_thread_count 2
    flush_interval 5s
    retry_forever
    retry_max_interval 30
    chunk_limit_size 2M
    queue_limit_length 8
    overflow_action block
  </buffer>
</match>

<match postgres.logs>
  @type elasticsearch
  @id elasticsearch_postgres
  host elasticsearch
  port 9200
  index_name postgres-logs
  type_name _doc
  logstash_format true
  logstash_prefix postgres-logs
  logstash_dateformat %Y.%m.%d
  include_tag_key true
  tag_key @log_name
  flush_interval 10s
  <buffer>
    @type file
    path /var/log/fluentd/postgres_buffer
    flush_mode interval
    retry_type exponential_backoff
    flush_thread_count 2
    flush_interval 5s
    retry_forever
    retry_max_interval 30
    chunk_limit_size 2M
    queue_limit_length 8
    overflow_action block
  </buffer>
</match>

<match redis.logs>
  @type elasticsearch
  @id elasticsearch_redis
  host elasticsearch
  port 9200
  index_name redis-logs
  type_name _doc
  logstash_format true
  logstash_prefix redis-logs
  logstash_dateformat %Y.%m.%d
  include_tag_key true
  tag_key @log_name
  flush_interval 10s
  <buffer>
    @type file
    path /var/log/fluentd/redis_buffer
    flush_mode interval
    retry_type exponential_backoff
    flush_thread_count 2
    flush_interval 5s
    retry_forever
    retry_max_interval 30
    chunk_limit_size 2M
    queue_limit_length 8
    overflow_action block
  </buffer>
</match>

<match systemd>
  @type elasticsearch
  @id elasticsearch_systemd
  host elasticsearch
  port 9200
  index_name system-logs
  type_name _doc
  logstash_format true
  logstash_prefix system-logs
  logstash_dateformat %Y.%m.%d
  include_tag_key true
  tag_key @log_name
  flush_interval 10s
  <buffer>
    @type file
    path /var/log/fluentd/systemd_buffer
    flush_mode interval
    retry_type exponential_backoff
    flush_thread_count 2
    flush_interval 5s
    retry_forever
    retry_max_interval 30
    chunk_limit_size 2M
    queue_limit_length 8
    overflow_action block
  </buffer>
</match>

# 备份输出到文件
<match **>
  @type file
  @id file_backup
  path /var/log/fluentd/backup/all_logs
  append true
  time_slice_format %Y%m%d
  time_slice_wait 10m
  time_format %Y%m%dT%H%M%S%z
  compress gzip
  <buffer time>
    timekey 1h
    timekey_wait 10m
    flush_mode interval
    flush_interval 30s
  </buffer>
</match>